{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook to evaluate multiple models \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have new models, make sure to delete all files that are in ../data/predictions folder before running this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from utils import TextSegmentationValidator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path().resolve()\n",
    "\n",
    "labels = [\n",
    "    \"H&E\", \"IHCplus\", \"IHC\", \"MOL\", \"CON\", \"ADV\", \"BRS\", \"RAD\", \"CLN\", \n",
    "    \"HIS\", \"SID\", \"UNR\", \"CAL\"\n",
    "]\n",
    "\n",
    "# Find all models that are in the model folder\n",
    "models = [model.name for model in BASE_DIR.joinpath(\"../models\").iterdir() if model.is_dir()]\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_list = []\n",
    "results = {}\n",
    "\n",
    "for model in models:\n",
    "    PREPROCESS_CONFIGURATION = model[model.find(\"_\"):]\n",
    "    \n",
    "    globals()[f\"evaluator{PREPROCESS_CONFIGURATION}\"] = TextSegmentationValidator(labels, model, BASE_DIR, \"../models\", \"../data/preprocessed_data\", \"../data/predictions\")\n",
    "\n",
    "    evaluator_list.append(globals()[f\"evaluator{PREPROCESS_CONFIGURATION}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for evaluator, model in zip(evaluator_list, models):\n",
    "    evaluator.validate('training')\n",
    "    evaluator.validate('validation')\n",
    "    \n",
    "    results[str(model)] = {}\n",
    "    results[str(model)]['training'] = {}\n",
    "    results[str(model)]['validation'] = {}\n",
    "\n",
    "    results[str(model)]['training']['weighted f1-score'] = evaluator.evaluation_results['training']['classification_report']['weighted avg']['f1-score']\n",
    "    results[str(model)]['training']['macro f1-score'] = evaluator.evaluation_results['training']['classification_report']['macro avg']['f1-score']\n",
    "    results[str(model)]['training']['accuracy'] = evaluator.evaluation_results['training']['accuracy']\n",
    "\n",
    "    results[str(model)]['validation']['weighted f1-score'] = evaluator.evaluation_results['validation']['classification_report']['weighted avg']['f1-score']\n",
    "    results[str(model)]['validation']['macro f1-score'] = evaluator.evaluation_results['validation']['classification_report']['macro avg']['f1-score']\n",
    "    results[str(model)]['validation']['accuracy'] = evaluator.evaluation_results['validation']['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the nested dictionary into a format suitable for DataFrame construction\n",
    "df_data = []\n",
    "for model, contexts in results.items():\n",
    "    for context, metrics in contexts.items():\n",
    "        row = {'Model': model, 'Dataset': context}\n",
    "        row.update(metrics)\n",
    "        df_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(df_data)\n",
    "df_results['Context'] = df_results['Model'].apply(lambda x: x.split(\"_\")[1]).astype(int)\n",
    "df_results['Header'] = df_results['Model'].apply(lambda x: x.split(\"_\")[3])\n",
    "df_results['Header'] = df_results['Header'].apply(lambda x: True if x == \"true\" else False)\n",
    "df_results['Oversample'] = df_results['Model'].apply(lambda x: \"_\".join(x.split(\"_\")[4:]))\n",
    "df_results = df_results.drop(columns=['Model'])\n",
    "df_results.sort_values(by=['Context', 'Header', 'Dataset'], inplace=True)\n",
    "df_results.set_index(['Context', 'Header', 'Oversample', 'Dataset'], inplace=True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.reset_index().set_index(['Context', 'Dataset', 'Header']).unstack().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_results.xs(\"training\", level=\"Dataset\").reset_index()\n",
    "df_val = df_results.xs(\"validation\", level=\"Dataset\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_header = df_results.xs(True, level='Header')\n",
    "df_results_no_header = df_results.xs(False, level='Header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_header_acc = df_results_header[['accuracy']].reset_index()\n",
    "df_results_header_f1 = df_results_header[['weighted f1-score']].reset_index()\n",
    "df_results_header_macro_f1 = df_results_header[['macro f1-score']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_no_header_acc = df_results_no_header[['accuracy']].reset_index()\n",
    "df_results_no_header_f1 = df_results_no_header[['weighted f1-score']].reset_index()\n",
    "df_results_no_header_macro_f1 = df_results_no_header[['macro f1-score']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted F1-score\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "max_train_header_f1 = df_results_header_f1[df_results_header_f1['Dataset'] == 'training']['weighted f1-score'].max()\n",
    "max_valid_header_f1 = df_results_header_f1[df_results_header_f1['Dataset'] == 'validation']['weighted f1-score'].max()\n",
    "\n",
    "max_train_no_header_f1 = df_results_no_header_f1[df_results_no_header_f1['Dataset'] == 'training']['weighted f1-score'].max()\n",
    "max_valid_no_header_f1 = df_results_no_header_f1[df_results_no_header_f1['Dataset'] == 'validation']['weighted f1-score'].max()\n",
    "\n",
    "# Plotting for the first subplot\n",
    "sns.barplot(data=df_results_header_f1, x='Context', y='weighted f1-score', hue='Dataset', palette='coolwarm', ax=ax1) \n",
    "ax1.set_title('Weighted F1-score for model including headers')  # Setting title for ax1\n",
    "ax1.set_xlabel('Context')  # Setting x-label for ax1\n",
    "ax1.set_ylabel('Weighted F1-score')  # Setting y-label for ax1\n",
    "ax1.legend(title='Dataset', loc='lower right')  # Setting legend for ax1\n",
    "# Add horizontal lines for highest training and validation scores\n",
    "ax1.axhline(max_train_header_f1, color='blue', linestyle='--', label='Max Training Weighted F1-score')\n",
    "ax1.axhline(max_valid_header_f1, color='orange', linestyle='--', label='Max Validation Weighted F1-score')\n",
    "ax1.set_ylim(0.75, 1)  # Setting y-limits for ax1\n",
    "\n",
    "# Plotting for the second subplot (assuming df_results_no_header_acc contains similar structure data for the model without headers)\n",
    "sns.barplot(data=df_results_no_header_f1, x='Context', y='weighted f1-score', hue='Dataset', palette='coolwarm', ax=ax2) \n",
    "ax2.set_title('Weighted F1-score for model excluding headers')  # Setting title for ax1\n",
    "ax2.set_xlabel('Context')  # Setting x-label for ax1\n",
    "ax2.set_ylabel('Weighted F1-score')  # Setting y-label for ax1\n",
    "ax2.legend(title='Dataset', loc='lower right')  # Setting legend for ax1\n",
    "# Add horizontal lines for highest training and validation scores\n",
    "ax2.axhline(max_train_no_header_f1, color='blue', linestyle='--', label='Max Training Weighted F1-score')\n",
    "ax2.axhline(max_valid_no_header_f1, color='orange', linestyle='--', label='Max Validation Weighted F1-score')\n",
    "ax2.set_ylim(0.75, 1)  # Setting y-limits for ax1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macro F1-score\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "max_train_header_macro_f1 = df_results_header_macro_f1[df_results_header_macro_f1['Dataset'] == 'training']['macro f1-score'].max()\n",
    "max_valid_header_macro_f1 = df_results_header_macro_f1[df_results_header_macro_f1['Dataset'] == 'validation']['macro f1-score'].max()\n",
    "\n",
    "max_train_no_header_macro_f1 = df_results_no_header_macro_f1[df_results_no_header_macro_f1['Dataset'] == 'training']['macro f1-score'].max()\n",
    "max_valid_no_header_macro_f1 = df_results_no_header_macro_f1[df_results_no_header_macro_f1['Dataset'] == 'validation']['macro f1-score'].max()\n",
    "\n",
    "# Plotting for the first subplot\n",
    "sns.barplot(data=df_results_header_macro_f1, x='Context', y='macro f1-score', hue='Dataset', palette='coolwarm', ax=ax1)\n",
    "ax1.set_title('Macro F1-score for model including headers')  # Setting title for ax1\n",
    "ax1.set_xlabel('Context')  # Setting x-label for ax1\n",
    "ax1.set_ylabel('Macro F1-score')  # Setting y-label for ax1\n",
    "ax1.legend(title='Dataset', loc='lower right')  # Setting legend for ax1\n",
    "# Add horizontal lines for highest training and validation scores\n",
    "ax1.axhline(max_train_header_macro_f1, color='blue', linestyle='--', label='Max Training Macro F1-score')\n",
    "ax1.axhline(max_valid_header_macro_f1, color='orange', linestyle='--', label='Max Validation Macro F1-score')\n",
    "ax1.set_ylim(0.75, 1)  # Setting y-limits for ax1\n",
    "\n",
    "# Plotting for the second subplot (assuming df_results_no_header_acc contains similar structure data for the model without headers)\n",
    "sns.barplot(data=df_results_no_header_macro_f1, x='Context', y='macro f1-score', hue='Dataset', palette='coolwarm', ax=ax2)\n",
    "ax2.set_title('Macro F1-score for model excluding headers')  # Setting title for ax2\n",
    "ax2.set_xlabel('Context')  # Setting x-label for ax2\n",
    "ax2.set_ylabel('Macro F1-score')  # Setting y-label for ax2\n",
    "ax2.legend(title='Dataset', loc='lower right')  # Setting legend for ax2\n",
    "# Add horizontal lines for highest training and validation scores\n",
    "ax2.axhline(max_train_no_header_macro_f1, color='blue', linestyle='--', label='Max Training Macro F1-score')\n",
    "ax2.axhline(max_valid_no_header_macro_f1, color='orange', linestyle='--', label='Max Validation Macro F1-score')\n",
    "ax2.set_ylim(0.75, 1)  # Setting y-limits for ax1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "max_train_header_acc = df_results_header_acc[df_results_header_acc['Dataset'] == 'training']['accuracy'].max()\n",
    "max_valid_header_acc = df_results_header_acc[df_results_header_acc['Dataset'] == 'validation']['accuracy'].max()\n",
    "\n",
    "max_train_no_header_acc = df_results_no_header_acc[df_results_no_header_acc['Dataset'] == 'training']['accuracy'].max()\n",
    "max_valid_no_header_acc = df_results_no_header_acc[df_results_no_header_acc['Dataset'] == 'validation']['accuracy'].max()\n",
    "\n",
    "# Plotting for the first subplot\n",
    "sns.barplot(data=df_results_header_acc, x='Context', y='accuracy', hue='Dataset', palette='coolwarm', ax=ax1) \n",
    "ax1.set_title('Accuracy for model including headers')  # Setting title for ax1\n",
    "ax1.set_xlabel('Context')  # Setting x-label for ax1\n",
    "ax1.set_ylabel('Accuracy')  # Setting y-label for ax1\n",
    "ax1.legend(title='Dataset', loc='lower right')  # Setting legend for ax1\n",
    "# Add horizontal lines for highest training and validation scores\n",
    "ax1.axhline(max_train_header_acc, color='blue', linestyle='--', label='Max Training Accuracy')\n",
    "ax1.axhline(max_valid_header_acc, color='orange', linestyle='--', label='Max Validation Accuracy')\n",
    "ax1.set_ylim(0.75, 1)  # Setting y-limits for ax1\n",
    "\n",
    "# Plotting for the second subplot (assuming df_results_no_header_acc contains similar structure data for the model without headers)\n",
    "sns.barplot(data=df_results_no_header_acc, x='Context', y='accuracy', hue='Dataset', palette='coolwarm', ax=ax2)\n",
    "ax2.set_title('Accuracy for model excluding headers')  # Setting title for ax2\n",
    "ax2.set_xlabel('Context')  # Setting x-label for ax2\n",
    "ax2.set_ylabel('Accuracy')  # Setting y-label for ax2\n",
    "ax2.legend(title='Dataset', loc='lower right')  # Setting legend for ax2\n",
    "# Add horizontal lines for highest training and validation scores\n",
    "ax2.axhline(max_train_no_header_acc, color='blue', linestyle='--', label='Max Training Accuracy')\n",
    "ax2.axhline(max_valid_no_header_acc, color='orange', linestyle='--', label='Max Validation Accuracy')\n",
    "ax2.set_ylim(0.75, 1)  # Setting y-limits for ax2\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
