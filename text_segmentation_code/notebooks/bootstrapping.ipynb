{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook that is used to calculate the confidence intervals for the performance of the model,\n",
    "using the bootstrap method.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Adds the project_root to the path\n",
    "\n",
    "from utils.validate_model import TextSegmentationValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "labels = [\n",
    "    \"H&E\", \"IHCplus\", \"IHC\", \"MOL\", \"CON\", \"ADV\", \"BRS\", \"RAD\", \"CLN\", \n",
    "    \"HIS\", \"SID\", \"UNR\", \"CAL\"\n",
    "]\n",
    "\n",
    "model_name = \"flan-t5-large-_context_7_headers_false_HIS_5_CAL_5\"\n",
    "\n",
    "BASE_DIR = Path().resolve().parent\n",
    "\n",
    "print(BASE_DIR)\n",
    "\n",
    "evaluation = TextSegmentationValidator(labels=labels,\n",
    "                        model_name=model_name,\n",
    "                        base_dir=BASE_DIR,\n",
    "                        model_dir=BASE_DIR / \"..\" / \"models\",\n",
    "                        data_preprocessed_dir=BASE_DIR / \"..\" / \"data\" / \"preprocessed_data\",\n",
    "                        data_predictions_dir=BASE_DIR / \"..\" / \"data\" / \"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.validate('validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.explain_errors(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = evaluation.actual_labels_per_medical_report, evaluation.predicted_labels_per_medical_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from sklearn.metrics import classification_report\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "def bootstrapper_p_medical_report(\n",
    "        y_true: Dict[str, List[str]],\n",
    "        y_pred: Dict[str, List[str]],\n",
    "        n_bootstraps: int = 1000,\n",
    "        seed: int = 42,\n",
    "        confidence_level: float = 0.95,\n",
    "        labels: List[str] = None,\n",
    "        include_all_labels_per_iteration: bool = False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if list(y_true.keys()) != list(y_pred.keys()):\n",
    "        raise ValueError(\"Keys of y_true and y_pred do not match\")\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_bootstrap_results = {}\n",
    "\n",
    "    for iter in tqdm.tqdm(range(n_bootstraps)):\n",
    "        random_keys = np.random.choice(list(y_true.keys()), size=len(y_true), replace=True)\n",
    "\n",
    "        # Only use bootstrapped samples that contain all labels\n",
    "        if include_all_labels_per_iteration:\n",
    "            # Adjusted check for label presence directly in loop condition for clarity\n",
    "            max_tries = 0\n",
    "            while len(set(flatten([y_true[key] for key in random_keys]))) != len(labels) and max_tries < 100:\n",
    "                random_keys = np.random.choice(list(y_true.keys()), size=len(y_true), replace=True)\n",
    "                max_tries += 1\n",
    "\n",
    "            if max_tries == 100:\n",
    "                raise ValueError(\"Unable to sample all labels in 100 tries\")\n",
    "\n",
    "        y_true_sample = flatten([y_true[key] for key in random_keys])\n",
    "        y_pred_sample = flatten([y_pred[key] for key in random_keys])\n",
    "\n",
    "        # Calculate precision, recall and f1-score\n",
    "        n_bootstrap_results[iter] = classification_report(y_true_sample, y_pred_sample, labels=labels, output_dict=True, zero_division=np.nan)\n",
    "\n",
    "    # calculate mean and confidence intervals for bootstrap samples\n",
    "    final_results_bootstrap = {}\n",
    "    appearances = {label: 0 for label in labels}\n",
    "\n",
    "    for label in labels:\n",
    "        final_results_bootstrap[label] = {}\n",
    "        for metric in ['precision', 'recall', 'f1-score']:\n",
    "            final_results_bootstrap[label][metric] = {}\n",
    "            values = [n_bootstrap_results[iter][label][metric] for iter in n_bootstrap_results.keys()]\n",
    "            values = [value for value in values if not np.isnan(value)]\n",
    "            mean = np.mean(values)\n",
    "            lower = np.quantile(values, (1-confidence_level)/2)\n",
    "            upper = np.quantile(values, 1-((1-confidence_level)/2))\n",
    "\n",
    "            final_results_bootstrap[label][metric]['mean'] = mean\n",
    "            final_results_bootstrap[label][metric]['lower'] = lower\n",
    "            final_results_bootstrap[label][metric]['upper'] = upper\n",
    "\n",
    "        appearances[label] = len(values)\n",
    "\n",
    "    columns = [\n",
    "    'Label', 'Precision Mean', 'Precision Lower CI', 'Precision Upper CI',\n",
    "    'Recall Mean', 'Recall Lower CI', 'Recall Upper CI',\n",
    "    'F1-Score Mean', 'F1-Score Lower CI', 'F1-Score Upper CI'\n",
    "    ]\n",
    "    # Process data into the new format\n",
    "    rows = []\n",
    "    for label, metrics in final_results_bootstrap.items():\n",
    "        row = [label]  # start with the label\n",
    "        for metric in ['precision', 'recall', 'f1-score']:\n",
    "            row.extend([metrics[metric]['mean'], metrics[metric]['lower'], metrics[metric]['upper']])\n",
    "        rows.append(row)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows, columns=columns).set_index('Label')\n",
    "\n",
    "    return df, appearances\n",
    "\n",
    "labels = [\n",
    "    \"<H&E>\", \"<IHCplus>\", \"<IHC>\", \"<MOL>\", \"<CON>\", \"<ADV>\", \"<BRS>\", \n",
    "    \"<RAD>\", \"<CLN>\", \"<HIS>\", \"<SID>\", \"<UNR>\", \"<CAL>\", \n",
    "    'micro avg', 'macro avg', 'weighted avg'\n",
    "]\n",
    "\n",
    "\n",
    "bootstrap_result, appearances = bootstrapper_p_medical_report(y_true, y_pred, n_bootstraps=10, labels=labels, include_all_labels_per_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "\n",
    "labels = [\n",
    "    \"<H&E>\", \n",
    "    \"<IHCplus>\",\n",
    "    \"<IHC>\", \n",
    "    \"<MOL>\", \n",
    "    \"<CON>\", \n",
    "    \"<ADV>\", \n",
    "    \"<BRS>\", \n",
    "    \"<RAD>\", \n",
    "    \"<CLN>\", \n",
    "    \"<HIS>\", \n",
    "    \"<SID>\", \n",
    "    \"<UNR>\", \n",
    "    \"<CAL>\",\n",
    "    \"macro avg\",\n",
    "    \"weighted avg\"\n",
    "]\n",
    "\n",
    "# Only select the rows with index in labels\n",
    "bootstrap_result.loc[labels]\n",
    "\n",
    "# Change the order of the rows\n",
    "labels = [\n",
    "    \"<H&E>\", \n",
    "    \"<IHC>\", \n",
    "    \"<IHCplus>\",\n",
    "    \"<MOL>\", \n",
    "    \"<CLN>\", \n",
    "    \"<HIS>\", \n",
    "    \"<CON>\", \n",
    "    \"<RAD>\", \n",
    "    \"<BRS>\", \n",
    "    \"<CAL>\", \n",
    "    \"<ADV>\", \n",
    "    \"<SID>\", \n",
    "    \"<UNR>\",\n",
    "    \"macro avg\",\n",
    "    \"weighted avg\"\n",
    "]\n",
    "\n",
    "# Remove all \"<\" and \">\" from the index\n",
    "bootstrap_result.loc[labels].rename(index=lambda x: x.replace(\"<\", \"\").replace(\">\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appearances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
